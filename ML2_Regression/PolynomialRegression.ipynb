{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e71043",
   "metadata": {},
   "source": [
    "# Polynomial Regression and Interaction Variables\n",
    "\n",
    "In this notebook, we are going to apply polynomial regression to the dataset of students grades vs. the time in hours they have spent studying and other factors presented in the previous notebook. We will also add interaction features to our training data to improve our predictions.\n",
    "\n",
    "FYI, the dataset is synthetic, I do not have any mean to monitor the time you spend studying my course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91498adb",
   "metadata": {},
   "source": [
    "## Importing the dependencies\n",
    "\n",
    "First, we are going to import all the dependencies that we will need for this lab. If you cannot run the following code cell, do not forget to [create an environment](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/), to install the dependencies inside of it (using the command `pip install -r requirements.txt`) and to use it as your Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c926879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./synthetic_student_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12776e98",
   "metadata": {},
   "source": [
    "## Example with a single input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4852b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.scatter(df[\"hours_studied\"], df[\"grade\"], c=\"b\", marker=\"+\")\n",
    "\n",
    "ax.set_xlabel(\"Number of hours studied\", fontsize=\"large\")\n",
    "ax.set_ylabel(\"Grade\", fontsize=\"large\")\n",
    "ax.set_ylim((0,20))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "X = df[\"hours_studied\"].to_numpy()\n",
    "Y = df[\"grade\"].to_numpy()\n",
    "\n",
    "poly_2 = PolynomialFeatures(2, include_bias=False)\n",
    "\n",
    "X_poly2 = poly_2.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "ax.scatter(X, Y, c=\"b\", marker=\"+\")\n",
    "\n",
    "ax.set_xlabel(\"Number of hours studied\", fontsize=\"large\")\n",
    "ax.set_ylabel(\"Grade\", fontsize=\"large\")\n",
    "ax.set_ylim((0,20))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly2, Y)\n",
    "\n",
    "# Predict line over full x range\n",
    "x_vals = np.linspace(df[\"hours_studied\"].min(), df[\"hours_studied\"].max(), 100).reshape(-1, 1)\n",
    "x_vals_poly2 = poly_2.fit_transform(x_vals)\n",
    "y_vals = model.predict(x_vals_poly2)\n",
    "\n",
    "ax.plot(x_vals, y_vals)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207cb74",
   "metadata": {},
   "source": [
    "### Example of an overfitted regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b16e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "X = df[\"hours_studied\"].to_numpy()\n",
    "Y = df[\"grade\"].to_numpy()\n",
    "\n",
    "poly_12 = PolynomialFeatures(12, include_bias=False)  # This time we compute all the powers of our feature up to 12\n",
    "\n",
    "X_poly12 = poly_12.fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "ax.scatter(X, Y, c=\"b\", marker=\"+\")\n",
    "\n",
    "ax.set_xlabel(\"Number of hours studied\", fontsize=\"large\")\n",
    "ax.set_ylabel(\"Grade\", fontsize=\"large\")\n",
    "ax.set_ylim((0,20))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly12, Y)\n",
    "\n",
    "# Predict line over full x range\n",
    "x_vals = np.linspace(df[\"hours_studied\"].min(), df[\"hours_studied\"].max(), 100).reshape(-1, 1)\n",
    "x_vals_poly12 = poly_12.fit_transform(x_vals)\n",
    "y_vals = model.predict(x_vals_poly12)\n",
    "\n",
    "ax.plot(x_vals, y_vals)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c440f",
   "metadata": {},
   "source": [
    "In this case, we needed to largely increase the number of powers because we had a single feature but this issue can happen much more quickly with additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc673a1",
   "metadata": {},
   "source": [
    "## Improved prediction with multiple variables and interaction\n",
    "\n",
    "Here, we will take into account the multiple parameters in our dataset. The results would be much more complex to plot so we will simply display the coefficients. We will also produce interaction variables to improve the quality of our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [\"hours_studied\", \"sleep_hours\", \"class_attendance\"]\n",
    "X = df[input_features].to_numpy()\n",
    "Y = df[\"grade\"].to_numpy()\n",
    "\n",
    "poly_2 = PolynomialFeatures(2, include_bias=False)\n",
    "X_poly2 = poly_2.fit_transform(X)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_poly2, Y, test_size=0.2, random_state=4321)\n",
    "\n",
    "scaler = StandardScaler()  # We have multiple features, do not forget to scale them\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # We scale our test data based on the mean and std of the training data to ensure that there is no imbalance between them\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "coeffs = model.coef_\n",
    "bias = model.intercept_\n",
    "print(f\"Regression equation (with scaled features): grade = {bias:.3f}{''.join([f' + {coeffs[i]:.3f} × {input_features[i]}' for i in range(len(input_features))])}\" \\\n",
    "      f\" + {coeffs[3]:.3f} × {input_features[0]}² + {coeffs[4]:.3f} × {input_features[0]} × {input_features[1]} + {coeffs[5]:.3f} × {input_features[0]} × {input_features[2]}\" \\\n",
    "      f\" + {coeffs[6]:.3f} × {input_features[1]}² + {coeffs[7]:.3f} × {input_features[1]} × {input_features[2]} + {coeffs[8]:.3f} × {input_features[2]}²\")\n",
    "\n",
    "score = model.score(X_test_scaled, Y_test)\n",
    "print(f\"R² score = {score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
